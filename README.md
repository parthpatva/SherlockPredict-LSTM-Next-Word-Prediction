This project demonstrates the implementation of a Natural Language Processing (NLP) model designed for next-word prediction. Built using TensorFlow and Keras, the model utilizes an LSTM (Long Short-Term Memory) architecture to understand and mimic the linguistic patterns of Arthur Conan Doyle.

Key Features:

Text Preprocessing: Tokenization and sequence padding of a text corpus.

Deep Learning Architecture: Utilizes Embedding layers, LSTM units, and Dropout layers to prevent overfitting.

Optimization: Implements Adam optimizer and Early Stopping to achieve the best validation loss.

Predictive Inference: A custom function that takes a seed string and generates the most statistically probable next word.

Visualization: Matplotlib integration to track training accuracy and loss over time.

Tech Stack
Language: Python 3

Frameworks: TensorFlow, Keras

Libraries: NumPy, Matplotlib, Scikit-learn

Environment: Google Colab / Jupyter Notebooks
